import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import google.generativeai as genai
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Configure Google Generative AI with API key
api_key = os.getenv("GOOGLE_API_KEY")
genai.configure(api_key=api_key)

def get_pdf_text(pdf_docs):
    """Extract text from uploaded PDF documents"""
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

def get_text_chunks(text):
    """Split text into manageable chunks for processing"""
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
    chunks = text_splitter.split_text(text)
    return chunks

def get_vector_store(text_chunks):
    """Create and save vector embeddings from text chunks"""
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
    vector_store.save_local("faiss_index")

def get_conversational_chain():
    """Create a Q&A chain with a Vietnamese-optimized prompt"""
    prompt_template = """
    B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh chuy√™n tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin t·ª´ t√†i li·ªáu. 
    H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch ch√≠nh x√°c, r√µ r√†ng v√† ƒë·∫ßy ƒë·ªß.
    
    N·ªôi dung t√†i li·ªáu:
    {context}
    
    C√¢u h·ªèi: 
    {question}
    
    Tr·∫£ l·ªùi:
    """
    
    model = ChatGoogleGenerativeAI(model="gemini-1.5-pro", temperature=0.3)
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
    return chain

def user_input(user_question):
    """Process user question and generate response from PDF content"""
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    
    try:
        new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
        docs = new_db.similarity_search(user_question)
        
        chain = get_conversational_chain()
        
        response = chain(
            {"input_documents": docs, "question": user_question},
            return_only_outputs=True
        )
        
        st.write("Tr·∫£ l·ªùi:", response["output_text"])
    except Exception as e:
        st.error(f"ƒê√£ x·∫£y ra l·ªói: {str(e)}")
        st.info("Vui l√≤ng t·∫£i l√™n t√†i li·ªáu PDF v√† nh·∫•n 'X·ª≠ l√Ω t√†i li·ªáu' tr∆∞·ªõc khi ƒë·∫∑t c√¢u h·ªèi.")

def main():
    """Main application function"""
    st.set_page_config(page_title="Chat PDF Ti·∫øng Vi·ªát", layout="wide")
    
    st.title("üáªüá≥ Tr√≤ chuy·ªán v·ªõi t√†i li·ªáu PDF b·∫±ng Gemini AI")
    st.markdown("---")
    
    with st.sidebar:
        st.header("üìÅ Qu·∫£n l√Ω t√†i li·ªáu")
        pdf_docs = st.file_uploader(
            "T·∫£i l√™n t√†i li·ªáu PDF c·ªßa b·∫°n", 
            accept_multiple_files=True,
            type=["pdf"]
        )
        
        process_button = st.button("X·ª≠ l√Ω t√†i li·ªáu")
        
        if process_button and pdf_docs:
            with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu..."):
                # Get PDF text
                raw_text = get_pdf_text(pdf_docs)
                
                # Get text chunks
                text_chunks = get_text_chunks(raw_text)
                
                # Create vector store
                get_vector_store(text_chunks)
                
                st.success("X·ª≠ l√Ω t√†i li·ªáu ho√†n t·∫•t! B·∫°n c√≥ th·ªÉ ƒë·∫∑t c√¢u h·ªèi ngay b√¢y gi·ªù.")
        
        st.markdown("---")
        st.markdown("### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:")
        st.info(
            "1. T·∫£i l√™n t√†i li·ªáu PDF\n"
            "2. Nh·∫•n 'X·ª≠ l√Ω t√†i li·ªáu'\n"
            "3. ƒê·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung t√†i li·ªáu"
        )
    
    # Chat interface
    st.header("üí¨ ƒê·∫∑t c√¢u h·ªèi v·ªÅ t√†i li·ªáu c·ªßa b·∫°n")
    user_question = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ t√†i li·ªáu PDF:")
    
    if user_question:
        user_input(user_question)
    
    # Display sample questions
    with st.expander("C√°c c√¢u h·ªèi m·∫´u"):
        st.markdown("""
        - N·ªôi dung ch√≠nh c·ªßa t√†i li·ªáu l√† g√¨?
        - T√≥m t·∫Øt th√¥ng tin quan tr·ªçng nh·∫•t trong t√†i li·ªáu.
        - Gi·∫£i th√≠ch kh√°i ni·ªám X ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong t√†i li·ªáu.
        - C√°c ƒëi·ªÉm ch√≠nh trong ph·∫ßn Y c·ªßa t√†i li·ªáu l√† g√¨?
        """)

if __name__ == "__main__":
    main()
